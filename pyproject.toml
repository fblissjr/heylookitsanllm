[build-system]
requires = ["setuptools>=70", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "heylookitsanllm"
version = "1.1.1"
description = "On-Device Multimodal LLM API server for local LLM models, using MLX and llama.cpp"
authors = [{ name = "Fred Bliss" }]
readme = "README.md"
license = { file = "LICENSE" }
requires-python = ">=3.11"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: MacOS",
    "Operating System :: POSIX :: Linux",
]

# -------------------------------------------------------------------
# Core runtime deps – array of PEP 508 strings (required by spec)
# -------------------------------------------------------------------
dependencies = [
    "fastapi",
    "uvicorn[standard]",
    "pydantic",
    "python-multipart",
    "pyyaml",
    "requests",
    "rich",
    "numpy",
    "Pillow",
    "tqdm",
]

# ─── Optional groups ────────────────────────────────────────────────────────
[project.optional-dependencies]
test = ["pytest>=8", "pytest-mock>=3"]
performance = [
    "orjson>=3.9.0",      # Fast JSON parsing
    "xxhash>=3.4.0",      # Fast hashing for caching
    "PyTurboJPEG>=1.7.0", # Fast JPEG encoding/decoding
    "uvloop>=0.19.0",     # Fast async event loop
    "cachetools>=5.3.0",  # Advanced caching
]
analytics = [
    "duckdb>=1.0.0", # Analytics database
]
mlx = [
    "mlx",                      # Core MLX library
    "mlx-lm",                   # MLX text generation
    "mlx-vlm",                  # MLX vision generation, may have problems on macos with scipy dependency, install manually or see README.md for installation instructions
    "datasets>=2.19.1",         # HuggingFace datasets
    "transformers>=4.53.0",     # HuggingFace transformers
    "opencv-python>=4.12.0.88", # Image processing for vision
    "torch",                    # PyTorch (required by some MLX models)
    "torchvision",              # Vision utilities
    # Note: mlx-vlm must be installed separately with: pip install git+https://github.com/Blaizzy/mlx-vlm.git --no-deps
]
llama-cpp = [
    "llama-cpp-python", # GGUF model support
]
all = ["heylookitsanllm[performance,analytics,mlx,llama-cpp]"]

[project.scripts]
heylookllm = "heylook_llm.server:main"

[tool.setuptools.package-dir]
"" = "src"

[tool.setuptools.packages.find]
where = ["src"]

[tool.setuptools.package-data]
"heylook_llm.templates" = ["*.jinja2", "*.tpl", "*.yaml"]
