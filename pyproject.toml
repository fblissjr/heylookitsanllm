[build-system]
requires = ["setuptools>=70", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "heylookitsanllm"
version = "1.1.1"
description = "On-Device Multimodal LLM API server for local LLM models, using MLX and llama.cpp"
authors = [{ name = "Fred Bliss" }]
readme = "README.md"
license = { file = "LICENSE" }
requires-python = ">=3.11"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: MacOS",
    "Operating System :: POSIX :: Linux",
]

# -------------------------------------------------------------------
# Core runtime deps – array of PEP 508 strings (required by spec)
# Security-patched versions with proper version pinning
# -------------------------------------------------------------------
dependencies = [
    "fastapi>=0.104.1",
    "uvicorn[standard]>=0.24.0",
    "pydantic>=2.5.0",
    "python-multipart>=0.0.6",
    "pyyaml>=6.0.1",
    "requests>=2.31.0",
    "rich>=13.0.0",
    "numpy>=1.24.0",
    "Pillow>=10.0.1",
    "tqdm>=4.65.0",
]

# ─── Optional groups ────────────────────────────────────────────────────────
[project.optional-dependencies]
test = ["pytest>=8", "pytest-mock>=3"]
performance = [
    "orjson>=3.9.15",      # Fast JSON parsing with security fixes
    "xxhash>=3.4.1",       # Fast hashing for caching
    "PyTurboJPEG>=1.7.5",  # Fast JPEG encoding/decoding
    "uvloop>=0.19.0",      # Fast async event loop
    "cachetools>=5.3.2",   # Advanced caching with thread safety improvements
    "imagecodecs>=2023.9.18", # Hardware-accelerated codecs
    "blake3>=0.4.0",       # Fastest cryptographic hash
    "diskcache>=5.6.3",    # Persistent disk cache
    "msgpack>=1.0.7",      # Binary serialization
    "aiofiles>=23.2.1",    # Async file I/O
    "aiocache>=0.12.2",    # Async caching
]
analytics = [
    "duckdb>=1.0.0", # Analytics database
]
profile = [
    "py-spy>=0.3.14",          # Sampling profiler
    "memray>=1.13.0",          # Memory profiler with security fixes
]
mlx = [
    "mlx",                      # Core MLX library
    "mlx-lm",                   # MLX text generation
    "mlx-vlm",                  # MLX vision generation
    "datasets>=2.19.1",         # HuggingFace datasets
    "transformers>=4.35.0",     # HuggingFace transformers
    "opencv-python>=4.8.1.78",  # Image processing for vision
    "torch>=2.0.0",
    "torchvision>=0.15.0",      # Vision utilities
    "scipy>=1.11.0",            # Required by mlx-vlm
    "soundfile>=0.12.1",        # Audio processing support
]
llama-cpp = [
    "llama-cpp-python>=0.2.0", # GGUF model support
]
all = ["heylookitsanllm[performance,analytics,mlx,llama-cpp,profile]"]

[project.scripts]
heylookllm = "heylook_llm.server:main"

[tool.setuptools.package-dir]
"" = "src"

[tool.setuptools.packages.find]
where = ["src"]

[tool.setuptools.package-data]
"heylook_llm.templates" = ["*.jinja2", "*.tpl", "*.yaml"]
