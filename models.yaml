# --- Global Server Settings ---
default_model: "gemma3n-e4b-it"
max_loaded_models: 1

# --- Model Definitions ---
models:
  # --- MLX Models ---
  - id: "qwen2.5-vl-72b-mlx"
    provider: "mlx"
    description: "Qwen2.5 VL Instruct mlx 72b"
    tags: ["large"]
    enabled: true
    config:
      model_path: "modelzoo/Qwen/Qwen2.5-VL-72B-Instruct-mlx-4bit"
      vision: true
      temperature: 1.0
      top_p: 0.95
      cache_type: "quantized"
      kv_bits: 4

  - id: "gemma3n-e4b"
    provider: "mlx"
    description: "Gemma 3n"
    tags: ["vision", "instruct", "gemma"]
    enabled: true
    config:
      model_path: "modelzoo/google/gemma-3n-E4B-bf16-mlx"
      vision: true
      temperature: 1.0
      top_p: 0.95

  - id: "gemma3n-e4b-it"
    provider: "mlx"
    description: "Gemma 3n"
    tags: ["vision", "instruct", "gemma"]
    enabled: true
    config:
      model_path: "modelzoo/google/gemma-3n-E4B-it-4bit-mlx-vision"
      vision: true
      temperature: 0.8
      top_p: 0.95

  - id: "mistral-small-mlx"
    provider: "mlx"
    description: "Mistral Small"
    tags: ["vision", "instruct", "mistral"]
    enabled: true
    config:
      model_path: "modelzoo/mistral/Mistral-Small-3.2-24B-Instruct-2506-MLX-8bit"
      vision: true
      temperature: 0.8
      top_p: 0.95

  - id: "gemma3-vision"
    provider: "mlx"
    description: "Gemma 3 vision"
    tags: ["vision", "instruct", "gemma"]
    enabled: true
    config:
      model_path: "modelzoo/google/gemma3-vision"
      vision: true
      temperature: 1.0
      top_p: 0.95

  # --- GGUF Models via llama.cpp ---
  - id: "qwen2.5-vl-72b-inst-gguf"
    provider: "llama_cpp"
    description: "Qwen2.5-VL 72B Instruct quantized GGUF"
    tags: ["vision", "large", "gguf", "qwen"]
    enabled: true
    config:
      model_path: "modelzoo/Qwen/Qwen2.5-VL-72B-Instruct-GGUF/Qwen2.5-VL-72B-Instruct-q4_0_l.gguf"
      mmproj_path: "modelzoo/Qwen/Qwen2.5-VL-72B-Instruct-GGUF/Qwen2.5-VL-72B-Instruct-mmproj-f16.gguf"
      chat_format: "qwen"
      n_gpu_layers: -1
      n_ctx: 4096
      vision: true
      cache_type: "normal"

  - id: "mistral-small-vision"
    provider: "llama_cpp"
    description: "Mistral Small 24B with vision capabilities and custom template"
    tags: ["vision", "mistral", "custom-template"]
    enabled: true
    config:
      model_path: "modelzoo/mistral/Mistral-Small-3.2-24B-Instruct-2506-GGUF/Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf"
      mmproj_path: "modelzoo/mistral/Mistral-Small-3.2-24B-Instruct-2506-GGUF/mmproj-Mistral-Small-3.2-24B-Instruct-2506-F16.gguf"
      chat_format_template: "templates/Mistral-Small-3.2-24B-Instruct-2506.jinja2"
      n_gpu_layers: -1
      n_ctx: 4096
      vision: true
      cache_type: "normal"

  - id: "Llama-3.2-1B-Instruct-4bit"
    provider: "mlx"
    description: "Llama-3.2-1B-Instruct-4bit"
    tags: ["small", "speculative", "fast", "text-only"]
    enabled: true
    config:
      model_path: "mlx-community/Llama-3.2-3B-Instruct-4bit"
      cache_type: "quantized"
      kv_bits: 4

  - id: "llama-3.1-8b-instruct"
    provider: "llama_cpp"
    description: "Llama 3.1 8B Instruct - GGUF version"
    tags: ["text-only", "instruct", "llama"]
    enabled: true
    config:
      model_path: "mlx-community/Llama-3.2-3B-Instruct-4bit"
      chat_format: "llama-3"
      n_gpu_layers: -1
      n_ctx: 8192
      vision: false
