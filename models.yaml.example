# models.yaml Configuration Reference
# =====================================
#
# PARAMETER DOCUMENTATION
# -----------------------
#
# MLX Provider Parameters (Apple Silicon optimized):
# ---------------------------------------------------
# REQUIRED:
#   model_path: string          # Path to MLX model directory
#   provider: "mlx"             # Must be "mlx" for MLX models
#
# OPTIONAL - Model Loading:
#   vision: boolean             # Enable vision capabilities (default: false)
#   trust_remote_code: boolean  # Allow remote code execution (default: false)
#
# OPTIONAL - Memory Management (critical for large models):
#   cache_type: string          # "standard" or "quantized" (default: "standard")
#                              # Use "quantized" for models > 30B params
#   kv_bits: integer           # Bits for KV cache: 4, 8 (default: 8)
#                              # 4-bit saves 75% memory, slight quality loss
#   kv_group_size: integer     # Quantization group size: 32-128 (default: 64)
#                              # Lower = faster, higher = better quality
#   quantized_kv_start: integer # Token position to start quantizing (default: 1024)
#                              # Keep first N tokens at full precision
#   max_kv_size: integer       # Maximum KV cache size in tokens (default: unlimited)
#                              # Limit to prevent OOM on large models
#
# OPTIONAL - Generation:
#   temperature: float         # 0.0-2.0, controls randomness (default: 0.7)
#   max_tokens: integer        # 1-32768, max generation length (default: 512)
#   top_p: float              # 0.0-1.0, nucleus sampling (default: 0.95)
#   top_k: integer            # 1-100, top-k sampling (default: 50)
#   min_p: float              # 0.0-1.0, min-p sampling (default: 0.05)
#                             # Often faster than top_p, disable top_p when using
#   repetition_penalty: float  # 1.0-2.0, penalize repetition (default: 1.0)
#   repetition_context_size: integer # Tokens to consider for repetition (default: 20)
#
# OPTIONAL - Optimization:
#   use_flash_attention: boolean # Enable Flash Attention (default: true)
#   batch_size: integer         # Batch size for processing (default: 1)
#   model_size_gb: integer      # Hint for Metal optimizer (actual model size)
#   draft_model_path: string    # Path to smaller draft model for speculative decoding
#   num_draft_tokens: integer   # Tokens to generate with draft model: 2-8 (default: 4)
#
# OPTIONAL - Queue Configuration:
#   enable_queue: boolean       # Enable request queuing for this model (default: false)
#   queue_batch_size: integer   # Max requests to batch together (default: 1)
#   queue_timeout_ms: integer   # Max wait time for batching (default: 100)
#
# GGUF Provider Parameters (llama.cpp backend):
# ---------------------------------------------
# REQUIRED:
#   model_path: string          # Path to .gguf file
#   provider: "gguf"            # Use "gguf" (or "llama_cpp" for backwards compatibility)
#
# OPTIONAL - Model Loading:
#   mmproj_path: string         # Path to vision projection .gguf (for multimodal)
#   chat_format: string         # Chat template: "chatml", "llama-2", "qwen", etc.
#   chat_format_template: string # Path to custom Jinja2 template file
#   embedding: boolean          # Enable embeddings extraction (default: false)
#
# OPTIONAL - Memory/GPU:
#   n_gpu_layers: integer       # GPU layers to offload: -1 for all (default: -1)
#                              # For M2 Ultra: -1 recommended for full GPU
#   n_ctx: integer             # Context window size: 512-32768 (default: 4096)
#                              # M2 Ultra can handle 32768 for smaller models
#   n_batch: integer           # Batch size: 1-2048 (default: 512)
#                              # Higher = faster prompt processing, more memory
#   use_mmap: boolean          # Memory-mapped file loading (default: true)
#                              # Faster loading, recommended for M2 Ultra
#   use_mlock: boolean         # Lock model in RAM (default: false)
#                              # Set true if you have spare RAM (M2 Ultra: yes)
#   n_threads: integer         # CPU threads: 1-64 (default: system/2)
#                              # M2 Ultra: 16-24 threads optimal
#
# OPTIONAL - Generation (same as MLX):
#   temperature, max_tokens, top_p, top_k, repeat_penalty
#
# M2 Ultra Optimization Recommendations (192GB RAM):
# --------------------------------------------------
# For MLX models:
#   - Models up to 30B: Use standard cache_type
#   - Models 30B-70B: Use quantized cache (kv_bits: 4-8)
#   - max_loaded_models: 1-2 (depending on model sizes)
#   - batch_size: 1 (streaming is typically single-request)
#   - use_flash_attention: true (always)
#
# For Llama.cpp models:
#   - n_gpu_layers: -1 (use all GPU layers)
#   - n_ctx: up to 32768 for models < 30B
#   - n_batch: 1024-2048 (leverage high memory)
#   - use_mlock: true (lock model in RAM)
#   - n_threads: 20-24 (optimal for M2 Ultra)
#
# Memory Usage Estimates:
#   - 7B model: ~7-14GB (4-bit to 8-bit)
#   - 13B model: ~13-26GB
#   - 30B model: ~30-60GB
#   - 70B model: ~70-140GB (use 4-bit quantization)
#
# --- Global Server Settings ---
default_model: "mistral-small-mlx"
max_loaded_models: 1 # For most use cases running on a single machine, set at 1 to minimize memory pressure and model swapping

# --- Queue Configuration (Optional) ---
# Enable request queuing and batching for improved throughput
queue_config:
  enabled: true # Set to true to enable queue manager
  max_batch_size: 4 # Maximum requests to batch together
  max_concurrent_batches: 2 # Maximum parallel batch processes
  batch_timeout_ms: 100 # Max wait time for batch formation
  max_queue_size: 100 # Maximum pending requests in queue
  enable_dynamic_batching: true # Adjust batch size based on load

# --- Model Definitions ---
models:
  # --- MLX Models ---
  - id: qwen2.5-vl-72b-mlx
    provider: "mlx"
    description: "Qwen2.5 VL Instruct mlx 72b - Massive model, needs optimization"
    tags: ["large", "vision", "72b"]
    enabled: true
    config:
      model_path: "modelzoo/Qwen/Qwen2.5-VL-72B-Instruct-mlx-4bit"
      vision: true

      # For 72B model, use quantized cache to save memory
      cache_type: "quantized" # ESSENTIAL for 72B model
      kv_bits: 4 # 4-bit KV cache reduces memory by 75%
      kv_group_size: 32 # Smaller groups for better speed
      quantized_kv_start: 512 # Start quantizing early
      max_kv_size: 2048 # Limit cache size to prevent OOM

      # Generation parameters
      temperature: 1.0
      max_tokens: 512

      # Sampling optimizations
      top_k: 40 # Reduced from 50 for faster sampling
      min_p: 0.05 # Min-p is sometimes faster than top-p
      # top_p: null           # Disable top-p when using min-p

      # Repetition penalty
      repetition_penalty: 1.05
      repetition_context_size: 20

      # Metal optimizations
      model_size_gb: 36 # Help Metal optimizer

      # Speculative decoding with the draft model you have
      draft_model_path: "modelzoo/Qwen/Qwen2.5-0.5B-Instruct-MLX-4bit"
      num_draft_tokens: 4 # Conservative for large model

  - id: dolphin-mistral
    provider: mlx
    description: dolphin mistral venice edition
    tags:
      - dolphin
      - mistral
    enabled: true
    config:
      model_path: modelzoo/Dolphin/Mistral-24B-Venice-Edition-DWQ
      vision: false
      temperature: 1.0
      cache_type: quantized
      kv_bits: 8
      kv_group_size: 64
      quantized_kv_start: 1024
      top_k: 40
      min_p: 0.05
      max_tokens: 512
      repetition_penalty: 1.05
      repetition_context_size: 20

  - id: "gemma3n-e4b-it"
    provider: "mlx"
    description: "Gemma 3n - Fast small model"
    tags: ["vision", "instruct", "gemma", "fast"]
    enabled: true
    config:
      model_path: "modelzoo/google/gemma-3n-E4B-it-4bit-mlx-vision"
      vision: true
      temperature: 0.8
      top_p: 0.95
      top_k: 50
      cache_type: "standard"
      use_flash_attention: true
      batch_size: 1

  - id: "mistral-small-mlx"
    provider: "mlx"
    description: "Mistral Small - Good balance"
    tags: ["vision", "instruct", "mistral"]
    enabled: true
    config:
      model_path: "modelzoo/mistral/Mistral-Small-3.2-24B-Instruct-2506-MLX-8bit"
      vision: true
      temperature: 0.7
      top_p: 0.95
      top_k: 50
      cache_type: "standard"
      use_flash_attention: true
      batch_size: 1

  # --- GGUF Models (for comparison) ---
  - id: "qwen2.5-vl-72b-gguf"
    provider: "gguf"
    description: "Qwen2.5 VL 72B GGUF version"
    tags: ["large", "gguf"]
    enabled: false # Enable when you have the file
    config:
      model_path: "modelzoo/Qwen/Qwen2.5-VL-72B-Instruct-Q4_K_M.gguf"
      n_ctx: 4096 # Context size
      n_gpu_layers: -1 # Use all GPU layers
      n_batch: 512 # Batch size for prompt processing
      temperature: 1.0
      top_p: 0.95
      top_k: 50
      repeat_penalty: 1.0

      # GGUF specific optimizations
      use_mmap: true # Memory-mapped file access
      use_mlock: false # Don't lock model in RAM
      n_threads: 8 # CPU threads for inference

      # Vision support for GGUF (if using llava format)
      # mmproj_path: "path/to/mmproj-model.gguf"

  # --- GGUF Models via llama.cpp ---
  - id: "qwen2.5-vl-72b-inst-gguf"
    provider: "gguf"
    description: "Qwen2.5-VL 72B Instruct quantized GGUF"
    tags: ["vision", "large", "gguf", "qwen"]
    enabled: true
    config:
      model_path: "modelzoo/Qwen/Qwen2.5-VL-72B-Instruct-GGUF/Qwen2.5-VL-72B-Instruct-q4_0_l.gguf"
      mmproj_path: "modelzoo/Qwen/Qwen2.5-VL-72B-Instruct-GGUF/Qwen2.5-VL-72B-Instruct-mmproj-f16.gguf"
      chat_format: "qwen"
      n_gpu_layers: -1
      n_ctx: 4096
      vision: true
      cache_type: "standard"

  - id: "mistral-small-vision"
    provider: "gguf"
    description: "Mistral Small 24B with vision capabilities and custom template"
    tags: ["vision", "mistral", "custom-template"]
    enabled: true
    config:
      model_path: "modelzoo/mistral/Mistral-Small-3.2-24B-Instruct-2506-GGUF/Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf"
      mmproj_path: "modelzoo/mistral/Mistral-Small-3.2-24B-Instruct-2506-GGUF/mmproj-Mistral-Small-3.2-24B-Instruct-2506-F16.gguf"
      chat_format_template: "templates/Mistral-Small-3.2-24B-Instruct-2506.jinja2"
      n_gpu_layers: -1
      n_ctx: 4096
      vision: true
      cache_type: "standard"

  - id: "Llama-3.2-1B-Instruct-4bit"
    provider: "mlx"
    description: "Llama-3.2-1B-Instruct-4bit"
    tags: ["small", "speculative", "fast", "text-only"]
    enabled: true
    config:
      model_path: "mlx-community/Llama-3.2-3B-Instruct-4bit"
      cache_type: "quantized"
      kv_bits: 4

  - id: "llama-3.1-8b-instruct"
    provider: "gguf"
    description: "Llama 3.1 8B Instruct - GGUF version"
    tags: ["text-only", "instruct", "llama"]
    enabled: true
    config:
      model_path: "mlx-community/Llama-3.2-3B-Instruct-4bit"
      chat_format: "llama-3"
      n_gpu_layers: -1
      n_ctx: 8192
      vision: false

  - id: gemma3-27b-it-4bit-DWQ-mlx
    provider: mlx
    description: Auto-imported MLX model (27B)
    tags:
      - quantized
      - gemma
    enabled: true
    config:
      model_path: modelzoo/mlx-community/gemma3-27b-it-4bit-DWQ-mlx
      vision: false
      temperature: 0.9
      cache_type: quantized
      kv_bits: 8
      kv_group_size: 64
      quantized_kv_start: 1024
      top_k: 40
      min_p: 0.05
      max_tokens: 512
      repetition_penalty: 1.05
      repetition_context_size: 20

  - id: medgemma-27b-text-it-8bit-mlx
    provider: mlx
    description: Auto-imported MLX model (27B)
    tags:
      - quantized
      - gemma
    enabled: true
    config:
      model_path: modelzoo/google/medgemma-27b-text-it-8bit-mlx
      vision: false
      temperature: 0.9
      cache_type: quantized
      kv_bits: 8
      kv_group_size: 64
      quantized_kv_start: 1024
      top_k: 40
      min_p: 0.05
      max_tokens: 512
      repetition_penalty: 1.05
      repetition_context_size: 20

  - id: qwen3-235B-mlx
    provider: mlx
    description: Auto-imported MLX model with vision (27B)
    enabled: true
    config:
      model_path: modelzoo/Qwen/Qwen3-235B-A22B-Instruct-2507-4bit-mlx
      vision: false
      temperature: 0.9
      cache_type: quantized
      kv_bits: 8
      kv_group_size: 64
      quantized_kv_start: 1024
      top_k: 40
      min_p: 0.05
      max_tokens: 512
      repetition_penalty: 1.05
      repetition_context_size: 20
