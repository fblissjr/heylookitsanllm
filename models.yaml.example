# Example models.yaml
# --- Global Server Settings ---
default_model: "gemma3n-e4b-it"
max_loaded_models: 1 # For most use cases running on a single machine, set at 1 to minimize memory pressure and model swapping

# --- Model Definitions ---
models:
  # --- MLX Models ---
  - id: qwen2.5-vl-72b-mlx
    provider: "mlx"
    description: "Qwen2.5 VL Instruct mlx 72b - Massive model, needs optimization"
    tags: ["large", "vision", "72b"]
    enabled: true
    config:
      model_path: "modelzoo/Qwen/Qwen2.5-VL-72B-Instruct-mlx-4bit"
      vision: true

      # For 72B model, use quantized cache to save memory
      cache_type: "quantized" # ESSENTIAL for 72B model
      kv_bits: 4 # 4-bit KV cache reduces memory by 75%
      kv_group_size: 32 # Smaller groups for better speed
      quantized_kv_start: 512 # Start quantizing early
      max_kv_size: 2048 # Limit cache size to prevent OOM

      # Generation parameters
      temperature: 1.0
      max_tokens: 512

      # Sampling optimizations
      top_k: 40 # Reduced from 50 for faster sampling
      min_p: 0.05 # Min-p is sometimes faster than top-p
      # top_p: null           # Disable top-p when using min-p

      # Repetition penalty
      repetition_penalty: 1.05
      repetition_context_size: 20

      # Metal optimizations
      model_size_gb: 36 # Help Metal optimizer

      # Speculative decoding with the draft model you have
      draft_model_path: "modelzoo/Qwen/Qwen2.5-0.5B-Instruct-MLX-4bit"
      num_draft_tokens: 4 # Conservative for large model

  - id: dolphin-mistral
    provider: mlx
    description: dolphin mistral venice edition
    tags:
      - dolphin
      - mistral
    enabled: true
    config:
      model_path: modelzoo/Dolphin/Mistral-24B-Venice-Edition-DWQ
      vision: false
      temperature: 1.0
      cache_type: quantized
      kv_bits: 8
      kv_group_size: 64
      quantized_kv_start: 1024
      top_k: 40
      min_p: 0.05
      max_tokens: 512
      repetition_penalty: 1.05
      repetition_context_size: 20

  - id: "gemma3n-e4b-it"
    provider: "mlx"
    description: "Gemma 3n - Fast small model"
    tags: ["vision", "instruct", "gemma", "fast"]
    enabled: true
    config:
      model_path: "modelzoo/google/gemma-3n-E4B-it-4bit-mlx-vision"
      vision: true
      temperature: 0.8
      top_p: 0.95
      top_k: 50
      cache_type: "standard"
      use_flash_attention: true
      batch_size: 1

  - id: "mistral-small-mlx"
    provider: "mlx"
    description: "Mistral Small - Good balance"
    tags: ["vision", "instruct", "mistral"]
    enabled: true
    config:
      model_path: "modelzoo/mistral/Mistral-Small-3.2-24B-Instruct-2506-MLX-8bit"
      vision: true
      temperature: 0.7
      top_p: 0.95
      top_k: 50
      cache_type: "standard"
      use_flash_attention: true
      batch_size: 1

  # --- GGUF Models (for comparison) ---
  - id: "qwen2.5-vl-72b-gguf"
    provider: "llama_cpp"
    description: "Qwen2.5 VL 72B GGUF version"
    tags: ["large", "gguf"]
    enabled: false # Enable when you have the file
    config:
      model_path: "modelzoo/Qwen/Qwen2.5-VL-72B-Instruct-Q4_K_M.gguf"
      n_ctx: 4096 # Context size
      n_gpu_layers: -1 # Use all GPU layers
      n_batch: 512 # Batch size for prompt processing
      temperature: 1.0
      top_p: 0.95
      top_k: 50
      repeat_penalty: 1.0

      # GGUF specific optimizations
      use_mmap: true # Memory-mapped file access
      use_mlock: false # Don't lock model in RAM
      n_threads: 8 # CPU threads for inference

      # Vision support for GGUF (if using llava format)
      # mmproj_path: "path/to/mmproj-model.gguf"

  # --- GGUF Models via llama.cpp ---
  - id: "qwen2.5-vl-72b-inst-gguf"
    provider: "llama_cpp"
    description: "Qwen2.5-VL 72B Instruct quantized GGUF"
    tags: ["vision", "large", "gguf", "qwen"]
    enabled: true
    config:
      model_path: "modelzoo/Qwen/Qwen2.5-VL-72B-Instruct-GGUF/Qwen2.5-VL-72B-Instruct-q4_0_l.gguf"
      mmproj_path: "modelzoo/Qwen/Qwen2.5-VL-72B-Instruct-GGUF/Qwen2.5-VL-72B-Instruct-mmproj-f16.gguf"
      chat_format: "qwen"
      n_gpu_layers: -1
      n_ctx: 4096
      vision: true
      cache_type: "standard"

  - id: "mistral-small-vision"
    provider: "llama_cpp"
    description: "Mistral Small 24B with vision capabilities and custom template"
    tags: ["vision", "mistral", "custom-template"]
    enabled: true
    config:
      model_path: "modelzoo/mistral/Mistral-Small-3.2-24B-Instruct-2506-GGUF/Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf"
      mmproj_path: "modelzoo/mistral/Mistral-Small-3.2-24B-Instruct-2506-GGUF/mmproj-Mistral-Small-3.2-24B-Instruct-2506-F16.gguf"
      chat_format_template: "templates/Mistral-Small-3.2-24B-Instruct-2506.jinja2"
      n_gpu_layers: -1
      n_ctx: 4096
      vision: true
      cache_type: "standard"

  - id: "Llama-3.2-1B-Instruct-4bit"
    provider: "mlx"
    description: "Llama-3.2-1B-Instruct-4bit"
    tags: ["small", "speculative", "fast", "text-only"]
    enabled: true
    config:
      model_path: "mlx-community/Llama-3.2-3B-Instruct-4bit"
      cache_type: "quantized"
      kv_bits: 4

  - id: "llama-3.1-8b-instruct"
    provider: "llama_cpp"
    description: "Llama 3.1 8B Instruct - GGUF version"
    tags: ["text-only", "instruct", "llama"]
    enabled: true
    config:
      model_path: "mlx-community/Llama-3.2-3B-Instruct-4bit"
      chat_format: "llama-3"
      n_gpu_layers: -1
      n_ctx: 8192
      vision: false

  - id: gemma3-27b-it-4bit-DWQ-mlx
    provider: mlx
    description: Auto-imported MLX model (27B)
    tags:
      - quantized
      - gemma
    enabled: true
    config:
      model_path: modelzoo/mlx-community/gemma3-27b-it-4bit-DWQ-mlx
      vision: false
      temperature: 0.9
      cache_type: quantized
      kv_bits: 8
      kv_group_size: 64
      quantized_kv_start: 1024
      top_k: 40
      min_p: 0.05
      max_tokens: 512
      repetition_penalty: 1.05
      repetition_context_size: 20

  - id: medgemma-27b-text-it-8bit-mlx
    provider: mlx
    description: Auto-imported MLX model (27B)
    tags:
      - quantized
      - gemma
    enabled: true
    config:
      model_path: modelzoo/google/medgemma-27b-text-it-8bit-mlx
      vision: false
      temperature: 0.9
      cache_type: quantized
      kv_bits: 8
      kv_group_size: 64
      quantized_kv_start: 1024
      top_k: 40
      min_p: 0.05
      max_tokens: 512
      repetition_penalty: 1.05
      repetition_context_size: 20

  - id: qwen3-235B-mlx
    provider: mlx
    description: Auto-imported MLX model with vision (27B)
    enabled: true
    config:
      model_path: modelzoo/Qwen/Qwen3-235B-A22B-Instruct-2507-4bit-mlx
      vision: false
      temperature: 0.9
      cache_type: quantized
      kv_bits: 8
      kv_group_size: 64
      quantized_kv_start: 1024
      top_k: 40
      min_p: 0.05
      max_tokens: 512
      repetition_penalty: 1.05
      repetition_context_size: 20
