/**
 * This file was auto-generated by openapi-typescript.
 * Do not make direct changes to the file.
 */

export interface paths {
    "/v1/audio/transcriptions": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Transcribe Audio
         * @description Transcribe audio file to text using the specified STT model.
         *
         *     **Supported formats:** WAV, MP3, M4A, FLAC, OGG, WebM
         *     **Max file size:** 25MB
         *     **Models:** parakeet-tdt-v3 (MLX)
         *
         *     This endpoint is compatible with OpenAI's Whisper API format.
         */
        post: operations["transcribe_audio_v1_audio_transcriptions_post"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/audio/translations": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Translate Audio
         * @description Transcribe and translate audio to English.
         *
         *     Currently uses the same model as transcription with translation capabilities.
         */
        post: operations["translate_audio_v1_audio_translations_post"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/stt/models": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /**
         * List STT Models
         * @description Get list of available speech-to-text models
         */
        get: operations["list_stt_models_v1_stt_models_get"];
        put?: never;
        post?: never;
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/models": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /**
         * List Available Models
         * @description List all language models currently available on this server.
         *
         *     **Use this endpoint to:**
         *     - Discover which models are loaded and ready for inference
         *     - Verify a specific model is available before making requests
         *     - Get model IDs for use in completion requests
         *
         *     **Returns:**
         *     - Model IDs (e.g., "qwen2.5-coder-1.5b-instruct-4bit")
         *     - OpenAI-compatible model objects
         *     - Only shows models marked as `enabled: true` in models.toml
         */
        get: operations["list_models_v1_models_get"];
        put?: never;
        post?: never;
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/system/metrics": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /**
         * Get System Metrics
         * @description Get current system resource and model metrics for monitoring dashboards.
         *
         *     **Returns:**
         *     - System metrics: RAM usage, CPU percentage
         *     - Per-model metrics: Context usage, memory, active requests
         *     - Cached for 30 seconds to minimize polling overhead
         *
         *     **Use Cases:**
         *     - Build monitoring dashboards
         *     - Track context window usage during conversations
         *     - Monitor system resource consumption
         *     - Alert on high memory/context usage
         *
         *     **Polling:**
         *     - Recommended poll interval: 5-10 seconds
         *     - Backend caches metrics for 30 seconds
         */
        get: operations["get_system_metrics_v1_system_metrics_get"];
        put?: never;
        post?: never;
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/chat/completions": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Create Chat Completion
         * @description Generate text completions from chat messages using the specified model.
         *
         *     **Key Features:**
         *     - Automatic model loading with LRU cache (max 2 models)
         *     - Vision model support with base64 images
         *     - Streaming responses (Server-Sent Events)
         *     - Batch processing for multiple prompts
         *     - Reproducible generation with seed parameter
         *     - Metal-optimized inference for MLX models
         *
         *     **Special Parameters:**
         *     - `processing_mode`: Control batch behavior ("sequential", "parallel", "conversation")
         *     - `return_individual`: Get separate responses for batch requests
         *     - `include_timing`: Add performance metrics to response
         *     - `stream`: Enable token-by-token streaming
         *
         *     **Performance Notes:**
         *     - First request to a model includes loading time (~2-30s depending on size)
         *     - Subsequent requests use cached model for instant inference
         *     - Vision models process images in parallel for efficiency
         */
        post: operations["create_chat_completion_v1_chat_completions_post"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/batch/chat/completions": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Batch Chat Completions
         * @description Process multiple chat completion requests in a single batch for improved throughput.
         *
         *     **Performance Benefits:**
         *     - 2-4x throughput improvement vs sequential processing
         *     - Efficient handling of variable-length prompts via left-padding
         *     - Optimized Metal memory management
         *
         *     **Requirements:**
         *     - All requests must use the same text-only model
         *     - Streaming is not supported (batch processing is inherently blocking)
         *     - Minimum 2 requests per batch (recommended 3+ for best performance)
         *
         *     **Batch Parameters:**
         *     - `completion_batch_size`: Max concurrent generations (default: 32)
         *     - `prefill_batch_size`: Max prefill parallelism (default: 8)
         *     - `prefill_step_size`: Chunk size for memory efficiency (default: 2048)
         *
         *     **Performance Notes:**
         *     - Best performance with similar-length prompts (reduces padding waste)
         *     - Larger batch sizes provide better throughput but higher latency
         *     - Monitor batch_stats in response for throughput metrics
         */
        post: operations["create_batch_chat_completion_v1_batch_chat_completions_post"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/admin/restart": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Restart Server
         * @description Restart the server to reload configuration and code changes.
         *
         *     **WARNING**: This will interrupt all active connections!
         *
         *     **Security Note**: This endpoint should be disabled in production or protected with authentication.
         */
        post: operations["restart_server_v1_admin_restart_post"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/admin/reload": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Reload Models
         * @description Reload model configuration and clear model cache without restarting the server.
         *
         *     This will:
         *     - Clear the loaded model cache
         *     - Reload models.toml configuration
         *     - Keep the server running
         */
        post: operations["reload_models_v1_admin_reload_post"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/embeddings": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Create Embeddings
         * @description Generate embeddings for text using the specified model.
         *
         *     **Key Features:**
         *     - Extract actual model embeddings (not hallucinated numbers)
         *     - Support for both text-only and vision models
         *     - Multiple pooling strategies (mean, cls, last, max)
         *     - Optional dimension truncation
         *     - Batch processing support
         *
         *     **Use Cases:**
         *     - Text similarity search
         *     - Semantic clustering
         *     - Cross-modal alignment
         *     - Prompt interpolation
         *     - Document retrieval
         *
         *     **Request Body:**
         *     - `input` (string | array[string]): Text(s) to embed
         *     - `model` (string): Model ID to use
         *     - `dimensions` (integer, optional): Truncate to N dimensions
         *     - `encoding_format` (string, optional): "float" or "base64"
         *     - `user` (string, optional): User identifier
         */
        post: operations["create_embeddings_endpoint_v1_embeddings_post"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/hidden_states": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Extract Hidden States
         * @description Extract raw hidden states from a specific layer of an LLM model.
         *
         *     **Key Differences from /v1/embeddings:**
         *     - Returns full sequence [seq_len, hidden_dim], not pooled
         *     - Extracts from specific layer (default: -2, second-to-last)
         *     - Filters out padding tokens via attention mask
         *     - Designed for use as text encoder backend for image generation
         *
         *     **Use Cases:**
         *     - Text encoder for DiT-based image generation (Z-Image, etc.)
         *     - Model interpretability and analysis
         *     - Cross-modal alignment with per-token embeddings
         *
         *     **Request Body:**
         *     - `input` (string | array[string]): Text(s) to encode (with chat template applied)
         *     - `model` (string): Model ID to use
         *     - `layer` (integer, optional): Layer to extract from (default: -2)
         *     - `max_length` (integer, optional): Max sequence length (default: 512)
         *     - `return_attention_mask` (boolean, optional): Include attention mask
         *     - `encoding_format` (string, optional): "float" (default) or "base64"
         *
         *     **Note:** Only supported for MLX models.
         */
        post: operations["extract_hidden_states_endpoint_v1_hidden_states_post"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/hidden_states/structured": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Extract Structured Hidden States
         * @description Extract hidden states with server-side chat template application and token boundary tracking.
         *
         *     **Key Differences from /v1/hidden_states:**
         *     - Accepts chat components separately (user_prompt, system_prompt, etc.)
         *     - Server applies Qwen3 chat template internally
         *     - Returns token boundary information for each section
         *     - Supports pre-filled thinking/assistant content
         *
         *     **Use Cases:**
         *     - Z-Image embeddings with precise template control
         *     - Token attribution research
         *     - Ablation studies on prompt sections
         *     - Debugging chat template formatting
         *
         *     **Request Body:**
         *     - `model` (string): Model ID to use
         *     - `user_prompt` (string): User message content (required)
         *     - `system_prompt` (string, optional): System prompt content
         *     - `thinking_content` (string, optional): Pre-filled thinking block
         *     - `assistant_content` (string, optional): Pre-filled assistant response
         *     - `enable_thinking` (boolean, optional): Control thinking mode (default: true)
         *     - `layer` (integer, optional): Layer to extract from (default: -2)
         *     - `max_length` (integer, optional): Max sequence length (default: 512)
         *     - `encoding_format` (string, optional): "float" (default) or "base64"
         *     - `return_token_boundaries` (boolean, optional): Return token indices per section
         *     - `return_formatted_prompt` (boolean, optional): Return formatted prompt string
         *
         *     **Note:** Only supported for MLX models with Qwen3-style chat templates.
         */
        post: operations["extract_structured_hidden_states_v1_hidden_states_structured_post"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/performance": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /**
         * Get Performance Metrics
         * @description Retrieve detailed performance metrics from all model providers.
         *
         *     **Metrics Include:**
         *     - ðŸ“Š Token generation rates (tokens/second)
         *     - â±ï¸ Time to first token (TTFT)
         *     - ðŸš€ Model loading times
         *     - ðŸ’¾ Memory usage statistics
         *     - ðŸ“ˆ Request processing times
         *     - ðŸŽ¯ Cache hit rates
         *
         *     **Per-Model Statistics:**
         *     - Request count
         *     - Average/peak token generation speed
         *     - Total tokens generated
         *     - Error rates
         *
         *     **Use Cases:**
         *     - Performance monitoring dashboards
         *     - Capacity planning
         *     - Model comparison
         *     - Troubleshooting slow inference
         *     - Identifying bottlenecks
         */
        get: operations["performance_metrics_v1_performance_get"];
        put?: never;
        post?: never;
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/data/summary": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /**
         * Get Analytics Summary
         * @description Get pre-computed analytics summary for dashboards.
         *
         *     **Note:** Analytics must be enabled via HEYLOOK_ANALYTICS_ENABLED=true
         *
         *     **Returns:**
         *     - total_requests: Total number of requests
         *     - avg_response_time: Average response time in ms
         *     - tokens_per_second: Average tokens per second
         *     - error_rate: Error rate as percentage
         *     - model_usage: Request count by model
         *     - recent_activity: Time-series data for charts
         */
        get: operations["data_summary_v1_data_summary_get"];
        put?: never;
        post?: never;
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/data/query": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Execute Analytics Query
         * @description Execute SQL queries against the analytics database.
         *
         *     **Note:** Analytics must be enabled via HEYLOOK_ANALYTICS_ENABLED=true
         *
         *     **Request body:**
         *     - query: SQL query to execute
         *     - limit: Maximum rows to return (default: 1000)
         *
         *     **Returns:**
         *     - columns: List of column names
         *     - data: Query results as list of rows
         *     - row_count: Number of rows returned
         *
         *     **Available tables:**
         *     - request_logs: Detailed request/response metrics
         *     - model_switches: Model loading/unloading events
         *     - performance_summary: Aggregated performance data
         *
         *     **Example queries:**
         *     - `SELECT * FROM request_logs WHERE total_time_ms > 1000`
         *     - `SELECT model, AVG(tokens_per_second) FROM request_logs GROUP BY model`
         */
        post: operations["data_query_v1_data_query_post"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/data/request/{request_id}": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /**
         * Get Request Details
         * @description Get detailed information about a specific request by ID.
         *
         *     **Note:** Analytics must be enabled with storage_level=full for complete data
         *
         *     **Returns:**
         *     - Full request details including messages
         *     - Response content
         *     - Timing breakdown
         *     - Token counts
         *     - Error information (if any)
         */
        get: operations["get_request_details_v1_data_request__request_id__get"];
        put?: never;
        post?: never;
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/replay/{request_id}": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Replay Request
         * @description Replay a previous request with optional parameter modifications.
         *
         *     **Note:** Analytics must be enabled for request history
         *
         *     **Request body (optional):**
         *     - model: Override the original model
         *     - temperature: Override temperature
         *     - max_tokens: Override max tokens
         *     - system_message: Add/override system message
         *
         *     **Returns:**
         *     - Original request details
         *     - Modified parameters
         *     - New response
         *     - Comparison metrics
         */
        post: operations["replay_request_v1_replay__request_id__post"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/eval/create": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Create Evaluation Set
         * @description Create a new evaluation set for model testing.
         *
         *     **Request body:**
         *     - name: Evaluation set name
         *     - description: Optional description
         *     - prompts: Array of evaluation prompts
         *       - prompt: The test prompt
         *       - expected_contains: Optional array of strings that should appear in response
         *       - expected_format: Optional format validation (json, code, markdown)
         *       - tags: Optional tags for categorization
         *
         *     **Returns:**
         *     - eval_id: Unique evaluation set ID
         *     - created_at: Creation timestamp
         *     - prompt_count: Number of test prompts
         */
        post: operations["create_eval_set_v1_eval_create_post"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/eval/run": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Run Evaluation
         * @description Run an evaluation set against one or more models.
         *
         *     **Request body:**
         *     - eval_id: Evaluation set ID
         *     - models: Array of model IDs to test
         *     - iterations: Number of iterations per prompt (default: 1)
         *     - parameters: Optional generation parameters
         *       - temperature: Override temperature
         *       - max_tokens: Override max tokens
         *
         *     **Returns:**
         *     - run_id: Unique run ID
         *     - status: Run status
         *     - progress: Progress information
         */
        post: operations["run_evaluation_v1_eval_run_post"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/eval/run/{run_id}": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /**
         * Get Evaluation Run Status
         * @description Get the status and results of an evaluation run.
         *
         *     **Returns:**
         *     - run_id: Run ID
         *     - status: Current status (running, completed, failed)
         *     - progress: Progress information
         *     - results: Results by model (when completed)
         */
        get: operations["get_evaluation_run_v1_eval_run__run_id__get"];
        put?: never;
        post?: never;
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/eval/list": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /**
         * List Evaluation Sets
         * @description List all available evaluation sets.
         *
         *     **Returns:**
         *     Array of evaluation sets with:
         *     - eval_id: Unique ID
         *     - name: Evaluation set name
         *     - description: Description
         *     - prompt_count: Number of prompts
         *     - created_at: Creation timestamp
         */
        get: operations["list_evaluation_sets_v1_eval_list_get"];
        put?: never;
        post?: never;
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/performance/profile/{time_range}": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /**
         * Get Performance Profile
         * @description Get detailed performance profiling data for the specified time range.
         *
         *     **Path parameters:**
         *     - time_range: Time range (1h, 6h, 24h, 7d)
         *
         *     **Returns:**
         *     - Timing breakdowns by operation type
         *     - Resource utilization over time
         *     - Bottleneck analysis
         *     - Performance trends
         */
        get: operations["get_performance_profile_v1_performance_profile__time_range__get"];
        put?: never;
        post?: never;
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/batch/process": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Batch Process Prompts
         * @description Process multiple prompts in batch with progress tracking.
         *
         *     **Request body:**
         *     - prompts: Array of prompt objects containing:
         *       - id: Unique identifier for the prompt
         *       - content: The prompt text
         *       - model: Model to use (optional, uses default if not specified)
         *       - temperature: Temperature override (optional)
         *       - max_tokens: Max tokens override (optional)
         *       - metadata: Any additional metadata (optional)
         *     - defaults: Default parameters for all prompts
         *       - model: Default model to use
         *       - temperature: Default temperature
         *       - max_tokens: Default max tokens
         *     - batch_config: Batch processing configuration
         *       - parallelism: Number of concurrent requests (default: 3)
         *       - retry_failed: Whether to retry failed requests (default: true)
         *       - max_retries: Maximum retry attempts (default: 2)
         *
         *     **Returns:**
         *     - batch_id: Unique batch processing ID
         *     - status: Current batch status
         *     - progress: Progress information
         */
        post: operations["batch_process_v1_batch_process_post"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/batch/{batch_id}": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /**
         * Get Batch Status
         * @description Get the status and results of a batch processing job.
         */
        get: operations["get_batch_status_v1_batch__batch_id__get"];
        put?: never;
        post?: never;
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/capabilities": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /**
         * Get Server Capabilities
         * @description Get detailed information about server capabilities and optimization options.
         *
         *     **Returns:**
         *     - Available performance optimizations
         *     - Supported features and endpoints
         *     - Optimal usage recommendations
         *     - API extensions
         *
         *     **Use this endpoint to:**
         *     - Discover fast endpoints (like multipart upload)
         *     - Check which optimizations are active
         *     - Get recommendations for best performance
         *     - Understand server limits and capabilities
         *
         *     **Client Integration:**
         *     Clients should query this endpoint on startup to discover:
         *     1. Whether to use `/v1/chat/completions/multipart` for images
         *     2. Recommended batch sizes
         *     3. Optimal request patterns
         *     4. Available performance features
         */
        get: operations["get_capabilities_v1_capabilities_get"];
        put?: never;
        post?: never;
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/cache/list": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /**
         * List Saved Prompt Caches
         * @description List all prompt caches currently in memory.
         *
         *     **Returns:**
         *     - List of cache entries with model ID and token counts
         *     - Cache statistics for each loaded model
         *
         *     **Note:** Currently shows in-memory caches only. Persistent storage coming soon.
         */
        get: operations["list_caches_v1_cache_list_get"];
        put?: never;
        post?: never;
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/cache/clear": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Clear Prompt Caches
         * @description Clear prompt caches for a specific model or all models.
         *
         *     **Use Cases:**
         *     - Free memory by clearing unused caches
         *     - Reset cache state when switching contexts
         *     - Troubleshooting cache-related issues
         *
         *     **Note:** This clears in-memory caches. The next request will rebuild the cache.
         */
        post: operations["clear_caches_v1_cache_clear_post"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/v1/chat/completions/multipart": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /**
         * Create Chat Completion with Raw Images (Fast)
         * @description High-performance vision endpoint that accepts raw image uploads instead of base64.
         *
         *     **ðŸš€ Performance Benefits:**
         *     - âš¡ 57ms faster per image (no base64 encoding/decoding)
         *     - ðŸ“‰ 33% bandwidth reduction
         *     - ðŸ”„ Parallel image processing
         *     - ðŸ’¾ Smart image caching with xxHash
         *
         *     **How to Use:**
         *     1. Send images as multipart form files
         *     2. Include messages as JSON string with `__RAW_IMAGE__` placeholders
         *     3. Images are injected into messages in order
         *
         *     **Example Request:**
         *     ```python
         *     files = [
         *         ('images', ('img1.jpg', image1_bytes, 'image/jpeg')),
         *         ('images', ('img2.jpg', image2_bytes, 'image/jpeg'))
         *     ]
         *     data = {
         *         'model': 'llava-1.5-7b-hf-4bit',
         *         'messages': json.dumps([{
         *             "role": "user",
         *             "content": [
         *                 {"type": "text", "text": "Compare these images"},
         *                 {"type": "image_url", "image_url": {"url": "__RAW_IMAGE__"}},
         *                 {"type": "image_url", "image_url": {"url": "__RAW_IMAGE__"}}
         *             ]
         *         }])
         *     }
         *     response = requests.post(url + '/multipart', files=files, data=data)
         *     ```
         *
         *     **Perfect for:**
         *     - ComfyUI integration
         *     - Batch image processing
         *     - Real-time vision applications
         *     - Network-constrained environments
         */
        post: operations["create_chat_multipart_v1_chat_completions_multipart_post"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /**
         * Server Information
         * @description Get server information and available endpoints
         */
        get: operations["root__get"];
        put?: never;
        post?: never;
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
}
export type webhooks = Record<string, never>;
export interface components {
    schemas: {
        /**
         * BatchChatRequest
         * @description Request for batch chat completions.
         */
        BatchChatRequest: {
            /** Requests */
            requests: components["schemas"]["ChatRequest"][];
            /**
             * Processing Mode
             * @default batch
             */
            processing_mode: string;
            /**
             * Completion Batch Size
             * @description Max concurrent generations
             * @default 32
             */
            completion_batch_size: number | undefined;
            /**
             * Prefill Batch Size
             * @description Max prefill parallelism
             * @default 8
             */
            prefill_batch_size: number | undefined;
            /**
             * Prefill Step Size
             * @description Chunk size for prefill
             * @default 2048
             */
            prefill_step_size: number | undefined;
        };
        /**
         * BatchChatResponse
         * @description Response for batch chat completions.
         */
        BatchChatResponse: {
            /**
             * Object
             * @default list
             */
            object: string;
            /** Data */
            data: components["schemas"]["ChatCompletionResponse"][];
            batch_stats: components["schemas"]["BatchStats"];
        };
        /**
         * BatchStats
         * @description Statistics for batch processing.
         */
        BatchStats: {
            /** Total Requests */
            total_requests: number;
            /** Elapsed Seconds */
            elapsed_seconds: number;
            /** Throughput Req Per Sec */
            throughput_req_per_sec: number;
            /** Throughput Tok Per Sec */
            throughput_tok_per_sec: number;
            /** Prefill Time */
            prefill_time: number;
            /** Generation Time */
            generation_time: number;
            /** Memory Peak Mb */
            memory_peak_mb: number;
        };
        /** Body_create_chat_multipart_v1_chat_completions_multipart_post */
        Body_create_chat_multipart_v1_chat_completions_multipart_post: {
            /** Model */
            model: string;
            /** Messages */
            messages: string;
            /** Temperature */
            temperature?: number | undefined;
            /** Top P */
            top_p?: number | undefined;
            /** Top K */
            top_k?: number | undefined;
            /** Min P */
            min_p?: number | undefined;
            /** Repetition Penalty */
            repetition_penalty?: number | undefined;
            /** Repetition Context Size */
            repetition_context_size?: number | undefined;
            /** Max Tokens */
            max_tokens?: number | undefined;
            /**
             * Stream
             * @default false
             */
            stream: boolean;
            /** Seed */
            seed?: number | undefined;
            /** Processing Mode */
            processing_mode?: string | undefined;
            /** Return Individual */
            return_individual?: boolean | undefined;
            /** Include Timing */
            include_timing?: boolean | undefined;
            /**
             * Resize Max
             * @description Resize images to max dimension (e.g., 512, 768, 1024)
             */
            resize_max?: number | undefined;
            /**
             * Resize Width
             * @description Resize images to specific width (overrides resize_max)
             */
            resize_width?: number | undefined;
            /**
             * Resize Height
             * @description Resize images to specific height (overrides resize_max)
             */
            resize_height?: number | undefined;
            /**
             * Image Quality
             * @description JPEG quality for resized images (1-100)
             * @default 85
             */
            image_quality: number | undefined;
            /**
             * Preserve Alpha
             * @description Preserve alpha channel (outputs PNG instead of JPEG)
             * @default false
             */
            preserve_alpha: boolean;
            /** Images */
            images?: string[];
        };
        /** Body_transcribe_audio_v1_audio_transcriptions_post */
        Body_transcribe_audio_v1_audio_transcriptions_post: {
            /**
             * File
             * Format: binary
             * @description Audio file to transcribe
             */
            file: string;
            /**
             * Model
             * @description Model ID to use
             */
            model: string;
            /**
             * Language
             * @description Language code
             */
            language?: string | undefined;
            /**
             * Prompt
             * @description Optional prompt
             */
            prompt?: string | undefined;
            /**
             * Response Format
             * @description Response format
             * @default json
             */
            response_format: string | undefined;
            /**
             * Temperature
             * @description Sampling temperature
             * @default 0
             */
            temperature: number | undefined;
        };
        /** Body_translate_audio_v1_audio_translations_post */
        Body_translate_audio_v1_audio_translations_post: {
            /**
             * File
             * Format: binary
             */
            file: string;
            /** Model */
            model: string;
            /** Prompt */
            prompt?: string | undefined;
            /**
             * Response Format
             * @default json
             */
            response_format: string | undefined;
            /**
             * Temperature
             * @default 0
             */
            temperature: number | undefined;
        };
        /**
         * CacheClearRequest
         * @description Request to clear caches.
         */
        CacheClearRequest: {
            /**
             * Model
             * @description Model ID to clear caches for (all if omitted)
             */
            model?: string | undefined;
        };
        /**
         * CacheClearResponse
         * @description Response from cache clear operation.
         */
        CacheClearResponse: {
            /** Deleted Count */
            deleted_count: number;
        };
        /**
         * CacheInfo
         * @description Information about a saved prompt cache.
         */
        CacheInfo: {
            /**
             * Cache Id
             * @description Unique cache identifier
             */
            cache_id: string;
            /**
             * Model
             * @description Model ID this cache belongs to
             */
            model: string;
            /**
             * Name
             * @description User-friendly cache name
             */
            name: string;
            /**
             * Description
             * @description Optional description
             */
            description?: string | undefined;
            /**
             * Tokens Cached
             * @description Number of tokens in cache
             */
            tokens_cached: number;
            /**
             * Size Mb
             * @description Cache file size in MB
             */
            size_mb: number;
            /**
             * Created At
             * @description ISO timestamp of creation
             */
            created_at: string;
        };
        /**
         * CacheListResponse
         * @description Response for listing saved caches.
         */
        CacheListResponse: {
            /** Caches */
            caches?: components["schemas"]["CacheInfo"][];
        };
        /** ChatCompletionResponse */
        ChatCompletionResponse: {
            /** Id */
            id: string;
            /** Object */
            object: string;
            /** Created */
            created: number;
            /** Model */
            model: string;
            /** Choices */
            choices: {
                [key: string]: unknown;
            }[];
            /** Usage */
            usage: {
                [key: string]: unknown;
            };
            performance?: components["schemas"]["PerformanceMetrics"] | undefined;
        };
        /** ChatMessage */
        ChatMessage: {
            /**
             * Role
             * @enum {string}
             */
            role: "system" | "user" | "assistant" | "tool";
            /** Content */
            content: string | (components["schemas"]["TextContentPart"] | components["schemas"]["ImageContentPart"])[];
            /** Name */
            name?: string | undefined;
            /** Tool Call Id */
            tool_call_id?: string | undefined;
            /** Tool Calls */
            tool_calls?: {
                [key: string]: unknown;
            }[] | undefined;
        };
        /** ChatRequest */
        ChatRequest: {
            /** Model */
            model?: string | undefined;
            /** Messages */
            messages: components["schemas"]["ChatMessage"][];
            /** Temperature */
            temperature?: number | undefined;
            /** Top P */
            top_p?: number | undefined;
            /** Top K */
            top_k?: number | undefined;
            /** Min P */
            min_p?: number | undefined;
            /** Repetition Penalty */
            repetition_penalty?: number | undefined;
            /** Repetition Context Size */
            repetition_context_size?: number | undefined;
            /** Max Tokens */
            max_tokens?: number | undefined;
            /**
             * Stream
             * @default false
             */
            stream: boolean;
            /**
             * Include Performance
             * @default false
             */
            include_performance: boolean;
            /** Seed */
            seed?: number | undefined;
            /**
             * Processing Mode
             * @description conversation|sequential|sequential_with_context
             */
            processing_mode?: string | undefined;
            /**
             * Return Individual
             * @description Return individual responses vs combined
             */
            return_individual?: boolean | undefined;
            /**
             * Include Timing
             * @description Include timing information
             */
            include_timing?: boolean | undefined;
            /**
             * Resize Max
             * @description Resize images to max dimension (e.g., 512, 768, 1024)
             */
            resize_max?: number | undefined;
            /**
             * Resize Width
             * @description Resize images to specific width
             */
            resize_width?: number | undefined;
            /**
             * Resize Height
             * @description Resize images to specific height
             */
            resize_height?: number | undefined;
            /**
             * Image Quality
             * @description JPEG quality for resized images
             */
            image_quality?: number | undefined;
            /**
             * Preserve Alpha
             * @description Preserve alpha channel (outputs PNG)
             */
            preserve_alpha?: boolean | undefined;
            /**
             * Enable Thinking
             * @description Enable thinking mode for Qwen3 models
             */
            enable_thinking?: boolean | undefined;
            /**
             * Presence Penalty
             * @description Reduce repetition (0-2, recommended 1.5 for Qwen3 thinking)
             */
            presence_penalty?: number | undefined;
            /**
             * Logprobs
             * @description Return log probabilities for output tokens
             */
            logprobs?: boolean | undefined;
            /**
             * Top Logprobs
             * @description Number of top tokens with log probabilities (0-20)
             */
            top_logprobs?: number | undefined;
            /**
             * Stream Options
             * @description Options for streaming: {include_usage: true} to get usage stats
             */
            stream_options?: {
                [key: string]: unknown;
            } | undefined;
        };
        /** HTTPValidationError */
        HTTPValidationError: {
            /** Detail */
            detail?: components["schemas"]["ValidationError"][];
        };
        /** ImageContentPart */
        ImageContentPart: {
            /**
             * Type
             * @constant
             */
            type: "image_url";
            image_url: components["schemas"]["ImageUrl"];
        };
        /** ImageUrl */
        ImageUrl: {
            /** Url */
            url: string;
        };
        /**
         * ModelMetrics
         * @description Per-model metrics (context usage, memory).
         */
        ModelMetrics: {
            /**
             * Context Used
             * @description Tokens currently in context
             */
            context_used: number;
            /**
             * Context Capacity
             * @description Maximum context window size
             */
            context_capacity: number;
            /**
             * Context Percent
             * @description Context usage percentage
             */
            context_percent: number;
            /**
             * Memory Mb
             * @description Model memory usage in MB
             */
            memory_mb: number;
            /**
             * Requests Active
             * @description Active requests for this model
             * @default 0
             */
            requests_active: number;
        };
        /** PerformanceMetrics */
        PerformanceMetrics: {
            /** Prompt Tps */
            prompt_tps: number;
            /** Generation Tps */
            generation_tps: number;
            /** Peak Memory Gb */
            peak_memory_gb: number;
        };
        /**
         * SystemMetricsResponse
         * @description Response for GET /v1/system/metrics endpoint.
         */
        SystemMetricsResponse: {
            /**
             * Timestamp
             * @description ISO timestamp of metrics collection
             */
            timestamp: string;
            system: components["schemas"]["SystemResourceMetrics"];
            /**
             * Models
             * @description Metrics per loaded model
             */
            models?: {
                [key: string]: components["schemas"]["ModelMetrics"];
            };
        };
        /**
         * SystemResourceMetrics
         * @description System-wide resource metrics (RAM, CPU).
         */
        SystemResourceMetrics: {
            /**
             * Ram Used Gb
             * @description RAM currently used in GB
             */
            ram_used_gb: number;
            /**
             * Ram Available Gb
             * @description RAM available in GB
             */
            ram_available_gb: number;
            /**
             * Ram Total Gb
             * @description Total system RAM in GB
             */
            ram_total_gb: number;
            /**
             * Cpu Percent
             * @description CPU usage percentage
             */
            cpu_percent: number;
        };
        /** TextContentPart */
        TextContentPart: {
            /**
             * Type
             * @constant
             */
            type: "text";
            /** Text */
            text: string;
        };
        /**
         * TranscriptionResponse
         * @description Response model for transcription.
         */
        TranscriptionResponse: {
            /**
             * Text
             * @description Transcribed text
             */
            text: string;
            /**
             * Language
             * @description Detected language
             */
            language?: string | undefined;
            /**
             * Duration
             * @description Audio duration in seconds
             */
            duration?: number | undefined;
            /**
             * Model
             * @description Model used for transcription
             */
            model?: string | undefined;
        };
        /** ValidationError */
        ValidationError: {
            /** Location */
            loc: (string | number)[];
            /** Message */
            msg: string;
            /** Error Type */
            type: string;
        };
        /**
         * EnhancedUsage
         * @description Extended usage statistics including thinking tokens.
         */
        EnhancedUsage: {
            /**
             * Prompt Tokens
             * @default 0
             */
            prompt_tokens: number;
            /**
             * Completion Tokens
             * @default 0
             */
            completion_tokens: number;
            /**
             * Thinking Tokens
             * @description Tokens used in thinking blocks
             * @default null
             */
            thinking_tokens: number | undefined;
            /**
             * Content Tokens
             * @description Tokens in actual content
             * @default null
             */
            content_tokens: number | undefined;
            /**
             * Total Tokens
             * @default 0
             */
            total_tokens: number;
        };
        /**
         * GenerationConfig
         * @description Sampler configuration used for generation.
         */
        GenerationConfig: {
            /**
             * Temperature
             * @default null
             */
            temperature: number | undefined;
            /**
             * Top P
             * @default null
             */
            top_p: number | undefined;
            /**
             * Top K
             * @default null
             */
            top_k: number | undefined;
            /**
             * Min P
             * @default null
             */
            min_p: number | undefined;
            /**
             * Enable Thinking
             * @default null
             */
            enable_thinking: boolean | undefined;
            /**
             * Max Tokens
             * @default null
             */
            max_tokens: number | undefined;
        };
        /**
         * GenerationTiming
         * @description Timing breakdown for generation phases.
         */
        GenerationTiming: {
            /**
             * Thinking Duration Ms
             * @description Time spent in thinking phase
             * @default null
             */
            thinking_duration_ms: number | undefined;
            /**
             * Content Duration Ms
             * @description Time spent generating content
             * @default null
             */
            content_duration_ms: number | undefined;
            /**
             * Total Duration Ms
             * @description Total generation time
             */
            total_duration_ms: number;
        };
        /**
         * StreamChoice
         * @description Single choice in a streaming chunk.
         */
        StreamChoice: {
            /**
             * Index
             * @default 0
             */
            index: number;
            delta?: components["schemas"]["StreamDelta"];
            /** @default null */
            logprobs: components["schemas"]["StreamLogprobs"] | undefined;
            /**
             * Finish Reason
             * @description 'stop', 'length', or null while streaming
             * @default null
             */
            finish_reason: string | undefined;
        };
        /**
         * StreamDelta
         * @description Delta content in a streaming chunk.
         */
        StreamDelta: {
            /**
             * Role
             * @description Role (only in first chunk)
             * @default null
             */
            role: string | undefined;
            /**
             * Content
             * @description Text content delta
             * @default null
             */
            content: string | undefined;
            /**
             * Thinking
             * @description Thinking content delta
             * @default null
             */
            thinking: string | undefined;
        };
        /**
         * StreamLogprobs
         * @description Logprobs attached to a streaming chunk.
         */
        StreamLogprobs: {
            /**
             * Content
             * @description Token-level logprob data for this chunk
             */
            content?: components["schemas"]["TokenLogprobInfo"][];
        };
        /**
         * TokenLogprobInfo
         * @description Token with its log probability and alternative candidates.
         */
        TokenLogprobInfo: {
            /**
             * Token
             * @description Token text
             */
            token: string;
            /**
             * Token Id
             * @description Token vocabulary ID
             */
            token_id: number;
            /**
             * Logprob
             * @description Log probability of this token
             */
            logprob: number;
            /**
             * Bytes
             * @description UTF-8 byte values
             */
            bytes?: number[];
            /**
             * Top Logprobs
             * @description Alternative tokens with their logprobs
             * @default null
             */
            top_logprobs: components["schemas"]["TopLogprobEntry"][] | undefined;
        };
        /**
         * TopLogprobEntry
         * @description A candidate token with its log probability (used in top_logprobs arrays).
         */
        TopLogprobEntry: {
            /**
             * Token
             * @description Token text
             */
            token: string;
            /**
             * Token Id
             * @description Token vocabulary ID
             */
            token_id: number;
            /**
             * Logprob
             * @description Log probability of this token
             */
            logprob: number;
            /**
             * Bytes
             * @description UTF-8 byte values
             */
            bytes?: number[];
        };
        /**
         * StreamChunk
         * @description SSE payload for a single streaming chunk (data: {...}).
         *
         *     Sent as Server-Sent Events on the /v1/chat/completions endpoint
         *     when stream=true. The final chunk includes usage, timing, and
         *     generation_config when stream_options.include_usage=true.
         */
        StreamChunk: {
            /**
             * Id
             * @description Response identifier (chatcmpl-...)
             */
            id: string;
            /**
             * Object
             * @default chat.completion.chunk
             * @constant
             */
            object: "chat.completion.chunk";
            /**
             * Created
             * @description Unix timestamp
             */
            created: number;
            /**
             * Model
             * @description Model ID used for generation
             */
            model: string;
            /** Choices */
            choices: components["schemas"]["StreamChoice"][];
            /**
             * @description Token usage (final chunk only, requires stream_options.include_usage)
             * @default null
             */
            usage: components["schemas"]["EnhancedUsage"] | undefined;
            /**
             * @description Generation timing breakdown (final chunk only)
             * @default null
             */
            timing: components["schemas"]["GenerationTiming"] | undefined;
            /**
             * @description Sampler settings used (final chunk only)
             * @default null
             */
            generation_config: components["schemas"]["GenerationConfig"] | undefined;
            /**
             * Stop Reason
             * @description Why generation stopped (final chunk only)
             * @default null
             */
            stop_reason: string | undefined;
        };
    };
    responses: never;
    parameters: never;
    requestBodies: never;
    headers: never;
    pathItems: never;
}
export type $defs = Record<string, never>;
export interface operations {
    transcribe_audio_v1_audio_transcriptions_post: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "multipart/form-data": components["schemas"]["Body_transcribe_audio_v1_audio_transcriptions_post"];
            };
        };
        responses: {
            /** @description Successful transcription */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["TranscriptionResponse"];
                };
            };
            /** @description Invalid audio format or parameters */
            400: {
                headers: {
                    [name: string]: unknown;
                };
                content?: never;
            };
            /** @description File too large */
            413: {
                headers: {
                    [name: string]: unknown;
                };
                content?: never;
            };
            /** @description Validation Error */
            422: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["HTTPValidationError"];
                };
            };
            /** @description Model loading or transcription error */
            500: {
                headers: {
                    [name: string]: unknown;
                };
                content?: never;
            };
        };
    };
    translate_audio_v1_audio_translations_post: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "multipart/form-data": components["schemas"]["Body_translate_audio_v1_audio_translations_post"];
            };
        };
        responses: {
            /** @description Successful Response */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["TranscriptionResponse"];
                };
            };
            /** @description Validation Error */
            422: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["HTTPValidationError"];
                };
            };
        };
    };
    list_stt_models_v1_stt_models_get: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Successful Response */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
        };
    };
    list_models_v1_models_get: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description List of available models in OpenAI-compatible format */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
        };
    };
    get_system_metrics_v1_system_metrics_get: {
        parameters: {
            query?: {
                force_refresh?: boolean;
            };
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Current system and model metrics */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["SystemMetricsResponse"];
                };
            };
            /** @description Validation Error */
            422: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["HTTPValidationError"];
                };
            };
        };
    };
    create_chat_completion_v1_chat_completions_post: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["ChatRequest"];
            };
        };
        responses: {
            /** @description Non-streaming: JSON response. Streaming (stream=true): Server-Sent Events where each `data:` line contains a StreamChunk JSON object, ending with `data: [DONE]`. */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["ChatCompletionResponse"];
                    "text/event-stream": {
                        /**
                         * Id
                         * @description Response identifier (chatcmpl-...)
                         */
                        id: string;
                        /**
                         * Object
                         * @default chat.completion.chunk
                         * @constant
                         */
                        object: "chat.completion.chunk";
                        /**
                         * Created
                         * @description Unix timestamp
                         */
                        created: number;
                        /**
                         * Model
                         * @description Model ID used for generation
                         */
                        model: string;
                        /** Choices */
                        choices: components["schemas"]["StreamChoice"][];
                        /** @description Token usage (final chunk only, requires stream_options.include_usage) */
                        usage?: components["schemas"]["EnhancedUsage"] | undefined;
                        /** @description Generation timing breakdown (final chunk only) */
                        timing?: components["schemas"]["GenerationTiming"] | undefined;
                        /** @description Sampler settings used (final chunk only) */
                        generation_config?: components["schemas"]["GenerationConfig"] | undefined;
                        /**
                         * Stop Reason
                         * @description Why generation stopped (final chunk only)
                         */
                        stop_reason?: string | undefined;
                        $defs: {
                            /**
                             * EnhancedUsage
                             * @description Extended usage statistics including thinking tokens.
                             */
                            EnhancedUsage: {
                                /**
                                 * Prompt Tokens
                                 * @default 0
                                 */
                                prompt_tokens: number;
                                /**
                                 * Completion Tokens
                                 * @default 0
                                 */
                                completion_tokens: number;
                                /**
                                 * Thinking Tokens
                                 * @description Tokens used in thinking blocks
                                 */
                                thinking_tokens?: number | undefined;
                                /**
                                 * Content Tokens
                                 * @description Tokens in actual content
                                 */
                                content_tokens?: number | undefined;
                                /**
                                 * Total Tokens
                                 * @default 0
                                 */
                                total_tokens: number;
                            };
                            /**
                             * GenerationConfig
                             * @description Sampler configuration used for generation.
                             */
                            GenerationConfig: {
                                /** Temperature */
                                temperature?: number | undefined;
                                /** Top P */
                                top_p?: number | undefined;
                                /** Top K */
                                top_k?: number | undefined;
                                /** Min P */
                                min_p?: number | undefined;
                                /** Enable Thinking */
                                enable_thinking?: boolean | undefined;
                                /** Max Tokens */
                                max_tokens?: number | undefined;
                            };
                            /**
                             * GenerationTiming
                             * @description Timing breakdown for generation phases.
                             */
                            GenerationTiming: {
                                /**
                                 * Thinking Duration Ms
                                 * @description Time spent in thinking phase
                                 */
                                thinking_duration_ms?: number | undefined;
                                /**
                                 * Content Duration Ms
                                 * @description Time spent generating content
                                 */
                                content_duration_ms?: number | undefined;
                                /**
                                 * Total Duration Ms
                                 * @description Total generation time
                                 */
                                total_duration_ms: number;
                            };
                            /**
                             * StreamChoice
                             * @description Single choice in a streaming chunk.
                             */
                            StreamChoice: {
                                /**
                                 * Index
                                 * @default 0
                                 */
                                index: number;
                                delta?: components["schemas"]["StreamDelta"];
                                logprobs?: components["schemas"]["StreamLogprobs"] | undefined;
                                /**
                                 * Finish Reason
                                 * @description 'stop', 'length', or null while streaming
                                 */
                                finish_reason?: string | undefined;
                            };
                            /**
                             * StreamDelta
                             * @description Delta content in a streaming chunk.
                             */
                            StreamDelta: {
                                /**
                                 * Role
                                 * @description Role (only in first chunk)
                                 */
                                role?: string | undefined;
                                /**
                                 * Content
                                 * @description Text content delta
                                 */
                                content?: string | undefined;
                                /**
                                 * Thinking
                                 * @description Thinking content delta
                                 */
                                thinking?: string | undefined;
                            };
                            /**
                             * StreamLogprobs
                             * @description Logprobs attached to a streaming chunk.
                             */
                            StreamLogprobs: {
                                /**
                                 * Content
                                 * @description Token-level logprob data for this chunk
                                 */
                                content?: components["schemas"]["TokenLogprobInfo"][];
                            };
                            /**
                             * TokenLogprobInfo
                             * @description Token with its log probability and alternative candidates.
                             */
                            TokenLogprobInfo: {
                                /**
                                 * Token
                                 * @description Token text
                                 */
                                token: string;
                                /**
                                 * Token Id
                                 * @description Token vocabulary ID
                                 */
                                token_id: number;
                                /**
                                 * Logprob
                                 * @description Log probability of this token
                                 */
                                logprob: number;
                                /**
                                 * Bytes
                                 * @description UTF-8 byte values
                                 */
                                bytes?: number[];
                                /**
                                 * Top Logprobs
                                 * @description Alternative tokens with their logprobs
                                 */
                                top_logprobs?: components["schemas"]["TopLogprobEntry"][] | undefined;
                            };
                            /**
                             * TopLogprobEntry
                             * @description A candidate token with its log probability (used in top_logprobs arrays).
                             */
                            TopLogprobEntry: {
                                /**
                                 * Token
                                 * @description Token text
                                 */
                                token: string;
                                /**
                                 * Token Id
                                 * @description Token vocabulary ID
                                 */
                                token_id: number;
                                /**
                                 * Logprob
                                 * @description Log probability of this token
                                 */
                                logprob: number;
                                /**
                                 * Bytes
                                 * @description UTF-8 byte values
                                 */
                                bytes?: number[];
                            };
                        };
                    };
                };
            };
            /** @description Validation Error */
            422: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["HTTPValidationError"];
                };
            };
        };
    };
    create_batch_chat_completion_v1_batch_chat_completions_post: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["BatchChatRequest"];
            };
        };
        responses: {
            /** @description Batch completion results with statistics */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["BatchChatResponse"];
                };
            };
            /** @description Validation Error */
            422: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["HTTPValidationError"];
                };
            };
        };
    };
    restart_server_v1_admin_restart_post: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Successful Response */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
        };
    };
    reload_models_v1_admin_reload_post: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Successful Response */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
        };
    };
    create_embeddings_endpoint_v1_embeddings_post: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": {
                    [key: string]: unknown;
                };
            };
        };
        responses: {
            /** @description Successful response */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
            /** @description Validation Error */
            422: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["HTTPValidationError"];
                };
            };
        };
    };
    extract_hidden_states_endpoint_v1_hidden_states_post: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": {
                    [key: string]: unknown;
                };
            };
        };
        responses: {
            /** @description Hidden states extracted successfully */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
            /** @description Model doesn't support hidden state extraction */
            422: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    /**
                     * @example {
                     *       "detail": "Hidden state extraction is not supported for this model."
                     *     }
                     */
                    "application/json": unknown;
                };
            };
        };
    };
    extract_structured_hidden_states_v1_hidden_states_structured_post: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": {
                    [key: string]: unknown;
                };
            };
        };
        responses: {
            /** @description Structured hidden states extracted successfully */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    /**
                     * @example {
                     *       "hidden_states": "SGVsbG8gV29ybGQ=",
                     *       "shape": [
                     *         120,
                     *         2560
                     *       ],
                     *       "model": "Qwen3-4B",
                     *       "layer": -2,
                     *       "dtype": "bfloat16",
                     *       "encoding_format": "base64",
                     *       "token_boundaries": {
                     *         "system": {
                     *           "start": 0,
                     *           "end": 35
                     *         },
                     *         "user": {
                     *           "start": 35,
                     *           "end": 80
                     *         }
                     *       },
                     *       "token_counts": {
                     *         "system": 35,
                     *         "user": 45,
                     *         "total": 120
                     *       }
                     *     }
                     */
                    "application/json": unknown;
                };
            };
            /** @description Model doesn't support structured hidden state extraction */
            422: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    /**
                     * @example {
                     *       "detail": "Structured hidden states only supported for MLX models."
                     *     }
                     */
                    "application/json": unknown;
                };
            };
        };
    };
    performance_metrics_v1_performance_get: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Detailed performance metrics and summaries */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
        };
    };
    data_summary_v1_data_summary_get: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Analytics summary data */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
        };
    };
    data_query_v1_data_query_post: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Query results with columns and data */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
        };
    };
    get_request_details_v1_data_request__request_id__get: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                request_id: string;
            };
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Complete request details */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
            /** @description Validation Error */
            422: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["HTTPValidationError"];
                };
            };
        };
    };
    replay_request_v1_replay__request_id__post: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                request_id: string;
            };
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Replay results with comparison */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
            /** @description Validation Error */
            422: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["HTTPValidationError"];
                };
            };
        };
    };
    create_eval_set_v1_eval_create_post: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Created evaluation set details */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
        };
    };
    run_evaluation_v1_eval_run_post: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Evaluation run details */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
        };
    };
    get_evaluation_run_v1_eval_run__run_id__get: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                run_id: string;
            };
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Evaluation run status and results */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
            /** @description Validation Error */
            422: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["HTTPValidationError"];
                };
            };
        };
    };
    list_evaluation_sets_v1_eval_list_get: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description List of evaluation sets */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
        };
    };
    get_performance_profile_v1_performance_profile__time_range__get: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                time_range: string;
            };
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Performance profiling data */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
            /** @description Validation Error */
            422: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["HTTPValidationError"];
                };
            };
        };
    };
    batch_process_v1_batch_process_post: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Batch processing status */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
        };
    };
    get_batch_status_v1_batch__batch_id__get: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                batch_id: string;
            };
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Batch processing status and results */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
            /** @description Validation Error */
            422: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["HTTPValidationError"];
                };
            };
        };
    };
    get_capabilities_v1_capabilities_get: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Server capabilities and optimization details */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
        };
    };
    list_caches_v1_cache_list_get: {
        parameters: {
            query?: {
                model?: string;
            };
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description List of cached prompts */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["CacheListResponse"];
                };
            };
            /** @description Validation Error */
            422: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["HTTPValidationError"];
                };
            };
        };
    };
    clear_caches_v1_cache_clear_post: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: {
            content: {
                "application/json": components["schemas"]["CacheClearRequest"];
            };
        };
        responses: {
            /** @description Number of caches cleared */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["CacheClearResponse"];
                };
            };
            /** @description Validation Error */
            422: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["HTTPValidationError"];
                };
            };
        };
    };
    create_chat_multipart_v1_chat_completions_multipart_post: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "multipart/form-data": components["schemas"]["Body_create_chat_multipart_v1_chat_completions_multipart_post"];
            };
        };
        responses: {
            /** @description Successful completion */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    /**
                     * @example {
                     *       "id": "chatcmpl-123",
                     *       "object": "chat.completion",
                     *       "created": 1677652288,
                     *       "model": "llava-1.5-7b-hf-4bit",
                     *       "choices": [
                     *         {
                     *           "index": 0,
                     *           "message": {
                     *             "role": "assistant",
                     *             "content": "The first image shows a cat, while the second shows a dog."
                     *           },
                     *           "finish_reason": "stop"
                     *         }
                     *       ],
                     *       "usage": {
                     *         "prompt_tokens": 156,
                     *         "completion_tokens": 23,
                     *         "total_tokens": 179
                     *       }
                     *     }
                     */
                    "application/json": unknown;
                };
            };
            /** @description Validation Error */
            422: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["HTTPValidationError"];
                };
            };
        };
    };
    root__get: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            /** @description Successful Response */
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": unknown;
                };
            };
        };
    };
}
