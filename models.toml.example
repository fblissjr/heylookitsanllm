# models.toml Configuration Reference
# =====================================
#
# PARAMETER DOCUMENTATION
# -----------------------
#
# MLX Provider Parameters (Apple Silicon optimized):
# ---------------------------------------------------
# REQUIRED:
#   model_path: string          # Path to MLX model directory
#   provider: "mlx"             # Must be "mlx" for MLX models
#
# OPTIONAL - Model Loading:
#   vision: boolean             # Enable vision capabilities (default: false)
#
# OPTIONAL - Memory Management (critical for large models):
#   cache_type: string          # "standard", "quantized", or "rotating" (default: "standard")
#                              # Use "quantized" for models > 30B params
#                              # Use "rotating" for fixed-size cache with auto-rotation
#   kv_bits: integer           # Bits for KV cache: 4, 8 (default: 8)
#                              # 4-bit saves 75% memory, slight quality loss
#   kv_group_size: integer     # Quantization group size: 32-128 (default: 64)
#                              # Lower = faster, higher = better quality
#   quantized_kv_start: integer # Token position to start quantizing (default: 2048)
#                              # Keep first N tokens at full precision
#   max_kv_size: integer       # Maximum KV cache size in tokens (default: unlimited)
#                              # Required for "rotating" cache type
#                              # Recommended for large models to prevent OOM
#
# OPTIONAL - Generation:
#   temperature: float         # 0.0-2.0, controls randomness (default: 0.7)
#   max_tokens: integer        # 1-32768, max generation length (default: 512)
#   top_p: float              # 0.0-1.0, nucleus sampling (default: 0.95)
#   top_k: integer            # 1-100, top-k sampling (default: 50)
#   min_p: float              # 0.0-1.0, min-p sampling (default: 0.05)
#                             # Often faster than top_p, disable top_p when using
#   repetition_penalty: float  # 1.0-2.0, penalize repetition (default: 1.0)
#   repetition_context_size: integer # Tokens to consider for repetition (default: 20)
#
# OPTIONAL - Optimization:
#   draft_model_path: string    # Path to smaller draft model for speculative decoding
#   num_draft_tokens: integer   # Tokens to generate with draft model: 2-8 (default: 6)
#
# GGUF Provider Parameters (llama.cpp backend):
# ---------------------------------------------
# REQUIRED:
#   model_path: string          # Path to .gguf file
#   provider: string            # "gguf" or "llama_cpp" (aliases)
#                              # "llama_server" for subprocess-based server (recommended for new installs)
#
# OPTIONAL - Model Loading:
#   mmproj_path: string         # Path to vision projection .gguf (for multimodal)
#   chat_format: string         # Chat template: "chatml", "llama-2", "llama-3", "qwen", "mistral", etc.
#   chat_format_template: string # Path to custom Jinja2 template file
#   embedding: boolean          # Enable embeddings extraction mode (default: false)
#                              # Required for /v1/embeddings endpoint with GGUF models
#
# OPTIONAL - Memory/GPU:
#   n_gpu_layers: integer       # GPU layers to offload: -1 for all (default: -1)
#   n_ctx: integer             # Context window size: 512-32768 (default: 4096)
#   n_batch: integer           # Batch size: 1-2048 (default: 512)
#   n_threads: integer         # CPU threads (default: auto-detect)
#   use_mmap: boolean          # Memory-mapped file loading (default: true)
#   use_mlock: boolean         # Lock model in RAM (default: false)
#
# OPTIONAL - Caching:
#   cache: boolean             # Enable RAM cache for repeated prompts (default: true)
#   cache_size: integer        # Cache size in bytes (default: 2147483648 = 2GB)
#
# OPTIONAL - llama_server provider specific:
#   parallel_slots: integer    # Number of concurrent request slots (default: 1)
#
# OPTIONAL - Generation (same as MLX):
#   temperature, max_tokens, top_p, top_k, min_p, repetition_penalty
#
# --- Global Server Settings ---
default_model = "mistral-small-mlx"
max_loaded_models = 1 # For most use cases running on a single machine, set at 1 to minimize memory pressure and model swapping

# --- Queue Configuration (Optional) ---
# Enable request queuing and batching for improved throughput
[queue_config]
enabled = true # Set to true to enable queue manager
max_batch_size = 4 # Maximum requests to batch together
max_concurrent_batches = 2 # Maximum parallel batch processes
batch_timeout_ms = 100 # Max wait time for batch formation
max_queue_size = 100 # Maximum pending requests in queue
enable_dynamic_batching = true # Adjust batch size based on load

# --- Model Definitions ---

# --- Encoder Models (for hidden states / text encoding) ---
# These models are optimized for the /v1/hidden_states endpoint used by
# image generation pipelines (Z-Image, etc.) that need text embeddings.
#
# NOTE: For hidden states extraction:
# - max_tokens does NOT limit hidden states (request's max_length does)
# - Temperature, top_k, etc. are IGNORED (no generation happens)
# - Only quantization level affects embedding precision
# - Use --profile encoder when importing models for this use case

[[models]]
id = "qwen3-4b-encoder"
provider = "mlx"
description = "Qwen3-4B optimized for hidden states extraction (Z-Image text encoder)"
tags = ["encoder", "qwen", "z-image"]
enabled = true

  [models.config]
  model_path = "modelzoo/Qwen/Qwen3-4B-mlx"
  vision = false
  cache_type = "standard"  # No quantization for maximum embedding precision
  max_tokens = 2048  # High limit (not used for hidden states, but allows long prompts if needed)
  # Note: Generation params below are ignored for /v1/hidden_states endpoint
  temperature = 0.7
  top_k = 50
  top_p = 0.95

# --- MLX Models ---
[[models]]
id = "qwen2.5-vl-72b-mlx"
provider = "mlx"
description = "Qwen2.5 VL Instruct mlx 72b - Massive model, needs optimization"
tags = ["large", "vision", "72b"]
enabled = true

  [models.config]
  model_path = "modelzoo/Qwen/Qwen2.5-VL-72B-Instruct-mlx-4bit"
  vision = true

  # For 72B model, use quantized cache to save memory
  cache_type = "quantized" # ESSENTIAL for 72B model
  kv_bits = 4 # 4-bit KV cache reduces memory by 75%
  kv_group_size = 32 # Smaller groups for better speed
  quantized_kv_start = 512 # Start quantizing early
  max_kv_size = 2048 # Limit cache size to prevent OOM

  # Generation parameters
  temperature = 1.0
  max_tokens = 512

  # Sampling optimizations
  top_k = 40 # Reduced from 50 for faster sampling
  min_p = 0.05 # Min-p is sometimes faster than top_p

  # Repetition penalty
  repetition_penalty = 1.05
  repetition_context_size = 20

  # Speculative decoding with draft model
  draft_model_path = "modelzoo/Qwen/Qwen2.5-0.5B-Instruct-MLX-4bit"
  num_draft_tokens = 4 # Conservative for large model

[[models]]
id = "mistral-small-mlx"
provider = "mlx"
description = "Mistral Small - Good balance"
tags = ["vision", "instruct", "mistral"]
enabled = true

  [models.config]
  model_path = "modelzoo/mistral/Mistral-Small-3.2-24B-Instruct-2506-MLX-8bit"
  vision = true
  temperature = 0.7
  top_p = 0.95
  top_k = 50
  cache_type = "standard"

# GGUF Models (llama-server provider recommended for new installations)
[[models]]
id = "qwen2.5-vl-72b-server"
provider = "llama_server"
description = "Qwen2.5 VL 72B via llama-server (subprocess)"
tags = ["vision", "large", "gguf"]
enabled = true

  [models.config]
  model_path = "modelzoo/Qwen/Qwen2.5-VL-72B-Instruct-GGUF/Qwen2.5-VL-72B-Instruct-q4_0_l.gguf"
  mmproj_path = "modelzoo/Qwen/Qwen2.5-VL-72B-Instruct-GGUF/Qwen2.5-VL-72B-Instruct-mmproj-f16.gguf"
  chat_format = "qwen"
  n_gpu_layers = -1
  n_ctx = 4096
  vision = true
  parallel_slots = 2 # Handle 2 concurrent requests
  temperature = 0.8
  max_tokens = 1024

[[models]]
id = "mistral-small-vision"
provider = "llama_server"
description = "Mistral Small 24B with vision capabilities"
tags = ["vision", "mistral", "custom-template"]
enabled = true

  [models.config]
  model_path = "modelzoo/mistral/Mistral-Small-3.2-24B-Instruct-2506-GGUF/Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf"
  mmproj_path = "modelzoo/mistral/Mistral-Small-3.2-24B-Instruct-2506-GGUF/mmproj-Mistral-Small-3.2-24B-Instruct-2506-F16.gguf"
  chat_format_template = "templates/Mistral-Small-3.2-24B-Instruct-2506.jinja2"
  n_gpu_layers = -1
  n_ctx = 4096
  vision = true
